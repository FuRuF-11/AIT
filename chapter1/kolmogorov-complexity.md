# Kolmogorov Complexity

## 前言

要想了解AIT，就要了解[柯氏复杂度](https://en.wikipedia.org/wiki/Kolmogorov_complexity)。

在上个世纪，有三位数学家（[所罗门诺夫](https://en.wikipedia.org/wiki/Ray_Solomonoff)，[柯尔莫戈洛夫](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)，[柴廷](https://en.wikipedia.org/wiki/Gregory_Chaitin)）分别在不同的时间点上独立的发现了柯氏复杂度（全称为柯尔莫戈洛夫复杂度，Kolmogorov Complexity，或者称为算法复杂度、描述复杂度、所罗门诺夫复杂度、柴廷常数）这个算法信息论中最核心的概念。

这三位数学家的出发点各不相同，但是从柯尔莫戈洛夫的视角出发，会更容易理解这个概念。

柯尔莫戈洛夫作为上世纪最伟大的苏联数学家，曾长久的为“信息的本质”这个问题所困扰。经过漫长的思考，柯尔莫戈洛夫在1963年左右得出了最终的结论。他认为，我们可以从三种不同的角度定义和理解信息的本质:
1. 组合学
2. 概率论
3. 算法 

其中，从组合学和概率论的角度出发，我们可以得到香农的经典信息论，也即从不确定性的角度来度量信息。而从算法的角度出发，我们可以得到柯氏复杂度。柯尔莫戈洛夫认为这个角度实际上是从随机性的角度来定义信息。同时他也认为，从这个角度出发得到的信息的定义是最好的。

## 经典信息论vs算法信息论

克劳德·香农在1948年发表了[《通信的数学原理》](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)，创立了经典信息论。从那个时刻开始，人们认识到信息不是一种虚无飘渺的东西。在香农的信息论中，信息就如同电压一样，是一种可以定量计算、有实际物理含义的物理量。香农认为，信息本质上是不确定性的一种体现。他用信息熵来定义信息。对于一个离散的随机变量$X$，其信息熵由如下数学公式进行计算：

$$
H(X):=-\sum_{x \in X}P(x)logP(x)
$$

对于连续的随机变量则需要使用积分的方法计算。但是不论如何，信息熵都是在随机变量的分布上进行计算的。所以从直观上我们很容易理解为何信息熵是与不确定性和概率相关的，因为信息熵需要使用随机变量不同取值下的概率进行计算。

为了能够更加直观的理解信息熵，也为了我们从组合的角度去理解信息，我们不妨思考一个信息传递问题。

这个问题可以这样描述：假设有A向B传递一条完全由英文写成的消息，B对A要发送的信息一无所知，如何确定A向B具体传送了多少信息？

如果我们将信息定义为对不确定性的度量，那么B现在对A所发出的信息有多么不确定？首先，英文字母一共有26个，假设A一共发送了N个字母且每一个字母出现概率是等可能的话，那么B可能会收到$26^N$种不同的消息，且每条消息被发出的概率都是$P(N)=\frac{1}{26^N}$。我们可以看到B对于将要接收到的信息具有巨大的不确定性。

每当B从A接收到一个字母，最终接收到的字符串的可能的组合都会减少，假设B已经收到i个字母，那么B最终收到的字符串的可能的组合减少为 $26^{N-i}$。这意味着每当B接受到了一个字母，B对于消息的不确定性就会减少。具体而言，组合数减少了 $26^i=\frac{26^{N}}{26^{N-i}}$，而不确定性也等量的减少了。因此我们可以从组合的角度来度量不确定性和信息。

但是如果我们使用组合数去定义不确定性的话，可能会在计算上十分的麻烦，不妨加上一个 $\log$。更具体一点，假设B收到了消息“abc”，我们可以直接计算B减少的不确定性也即 
$$
I(abc)=-\log P(abc)=-\log \frac{1}{26^3}=i\log26
$$
其中$I()$表示信息。

这样的设置有诸多的好处，例如我们可以直接使用加法计算收到“a”然后再收到“b”的减少的不确定性，即$I(ab)=I(a)+I(b)=-\log P(a)-\log P(b)=-\log P(ab)$。

再来做进一步的思考，尽管我们假设B收到了消息“abc”，但这不是B唯一可能接收到的信息。我们应当平均考虑B能够接收到的消息的所有可能性，也即B所消除的不确定性的期望来作为信息，即：$I(X)=E(X)=-P(X)\log{P(X)}=\frac{1}{26^N}N\log26$，其中N为收到的消息X的长度。

从这里，我们就可以直观的理解为何香农要使用概率和组合学的方式去定义信息了。每一条消息的发送，我们都可以认为是消除了接收者对于消息的不确定性，而接收者对于信息的不确定恰好可以用消息所有可能的组合或者说消息的概率去进行度量。

好了，现在不管从那个角度来看，香农的信息论都看起来天衣无缝，不确定性似乎是信息的天然度量。

但是不确定性真的就是**信息**这个词的全部了吗？

柯尔莫戈洛夫对此持有不同态度，他认为信息的本质实际上是**随机性**。

随机性和不确定性似乎非常相似，但是两者并非完全等价的关系。



