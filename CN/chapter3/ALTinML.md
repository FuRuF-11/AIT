# 为什么要有算法信息论？

作为达特茅斯会议最初的十位参与者，所罗门诺夫发明算法概率的初衷，是希望能从归纳法的角度解决达特茅斯会议上遗留下的问题：给定一个序列的初始段，如何预测该序列的后续部分？

这个问题看似简单，实则极为深刻。想象一下，如果初始序列是“黎曼假设可以通过如下方法证明：”，那么要想正确的预测后续部分，实际上需要你完成对于黎曼猜想的证明！

这实际上告诉我们说：任意的一个数学问题，都可以归化为给定初始序列，然后预测序列后半部分的格式。图灵曾严谨的用图灵机证明了这点。

但是你也可以想到，解决这个给定序列预测后续的问题可比制造人工智能困难多了，因为说到底人工智能充其量只能算是所有数学问题中的一小部分。

但是在所罗门诺夫看来，只要能解决这个问题，我们自然就解决了人工智能的问题，所以为什么不直接先解决它？在他看来，所谓的给定前缀预测后续问题，不过是一个归纳推理的过程，而这正是他擅长的领域。

然后，所罗门诺夫在这个深邃的问题上独自思考了10年，并最终给出了这个问题的理论的最优解：**所罗门诺夫归纳法**。而算法信息论则是他漫长思考过程的副产物，是解决最终问题是脚手架。

由此可见，算法信息论就是为了解决机器学习的终极问题而生。不夸张的说，算法信息论与人工智能这两件事物是伴生的关系。

因此接下来，我将尝试利用AIT的理论，系统的将各类零散的机器学习算法组织起来，用一个统一的理论框架展示来他们共同的数学原理。


## 从压缩的视角看机器学习

学习（learning）是一个抽象的动词，这注定我们可以有无数种方法和视角来看待机器学习这个领域。这种特性使得机器学习理论变成了一个多样到有些繁杂的领域。但是实际上，在压缩的视角下，机器学习这件事情就如同阳光下的一片叶子一样有着的清晰的主干和脉络。因为从算法信息论的角度来看，学习这件事情本质上就是在压缩数据。


## 最短描述长度

AIT在机器学习中的最广为人知的理论贡献是最短描述长度。


## 神经网络与所罗门诺夫归纳法

神经网络的泛化能力来源于其内部对于柯氏复杂度较低的函数的偏好。


## 柏拉图假说


<!-- ## 柏拉图的洞穴寓言

在柏拉图的洞穴寓言中，一些可怜的囚犯从出生开始就锁在一个漆黑的洞穴中。他们所有人都被锁在一面石壁前，不能转头，只能看着墙壁。这个洞穴中还有一群守卫，他们在这些囚犯身后点起了一堆火，囚犯之间是一条高高的带有矮墙的人行道，守卫则在手里拿着“人和其他生物”的物品或木偶在道上行走。

对于这些被锁住的可怜囚犯来说，那些墙上的阴影就是他们的整个世界。他们对于整个世界的认知都来源于这些影子。

作为生活在现代的自由人，你也许很难想象这些囚犯是如何认知整个世界的。但是实际上从某种意义上来讲，我们也是一群被锁在石壁前的囚犯。

实际上，当我们观察到某件事情发生时，我们实际上也是在通过火炬在墙上的投影来观察这个世界。这些投影对于那些囚犯来说是墙上的影子，对我们来说则是我们听到的声音、看到的光线和触碰时的压感等等。

可以说对于发生在现实世界中的事件，我们仅仅只是通过我们的神经器官观察到了其部分结果，而这些事件背后发生的具体原因对于我们其实有相当一部分是未知的。

我们能否直接的通过感官来认识到这个世界背后的运行规律？我想这是极为困难的。对于我们来说，万事万物的背后都有无穷的可能性，而我们有限的感官无法接受所有的信息，也无法对所有的可能性进行判断。

但是有一点我们可以确信，那就是一件事只要发生就必然有导致它发生的唯一原因。正如洞穴中不同的囚犯从不同角度看到相同物体的不同影子。

也正是为了处理这种万事万物的背后的不确定性，概率论诞生了。我们将所有这些事件组成了一个事件空间，并根据据此赋予了每个事件一个概率。在大数定律的规约下，我们可以确信，这些事件的发生的概率最终会符合某种分布的描述。

而算法概率与传统概率的不同就从这里开始。 -->


在机器学习的研究领域中，有学者类比柏拉图的洞穴寓言提出了[柏拉图假说](https://phillipi.github.io/prh/)（Platonic Hypothesis）。

这个假说认为：**在不同数据和模式上以不同目标进行训练的神经网络正在其表示空间中收敛为现实的共享统计模型。**
