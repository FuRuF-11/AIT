# Kolmogorov Complexity

## 前言

要想了解AIT，就要了解[柯氏复杂度](https://en.wikipedia.org/wiki/Kolmogorov_complexity)。

在上个世纪，有三位数学家（[所罗门诺夫](https://en.wikipedia.org/wiki/Ray_Solomonoff)，[柯尔莫戈洛夫](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)，[柴廷](https://en.wikipedia.org/wiki/Gregory_Chaitin)）分别在不同的时间点上独立的发现了柯氏复杂度（全称为柯尔莫戈洛夫复杂度，Kolmogorov Complexity，或者称为算法复杂度、描述复杂度、所罗门诺夫复杂度、柴廷常数）这个算法信息论中最核心的概念。

这三位数学家的出发点各不相同，但是从柯尔莫戈洛夫的视角出发，会更容易理解这个概念。

柯尔莫戈洛夫作为上世纪最伟大的苏联数学家，曾长久的为“信息的本质”这个问题所困扰。经过漫长的思考，柯尔莫戈洛夫在1963年左右得出了最终的结论。他认为，我们可以从三种不同的角度定义和理解信息的本质:
1. 组合学
2. 概率论
3. 算法 

其中，从组合学和概率论的角度出发，我们可以得到香农的经典信息论，也即从不确定性的角度来度量信息。而从算法的角度出发，我们可以得到柯氏复杂度。柯尔莫戈洛夫认为这个角度实际上是从随机性的角度来定义信息。同时他也认为，从这个角度出发得到的信息的定义是最好的。

## 经典信息论vs算法信息论

克劳德·香农在1948年发表了[《通信的数学原理》](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)，创立了经典信息论。从那个时刻开始，人们认识到信息不是一种虚无飘渺的东西。在香农的信息论中，信息就如同电压一样，是一种可以定量计算、有实际物理含义的物理量。香农认为，信息本质上是不确定性的一种体现。他用信息熵来定义信息。对于一个离散的随机变量$X$，其信息熵由如下数学公式进行计算：

$$
H(X):=-\sum_{x \in X}P(x)logP(x)
$$

对于连续的随机变量则需要使用积分的方法计算。但是不论如何，信息熵都是在随机变量的分布上进行计算的。所以从直观上我们很容易理解为何信息熵是与不确定性和概率相关的，因为信息熵需要使用随机变量不同取值下的概率进行计算。

为了能够更加直观的理解信息熵，也为了我们从组合的角度去理解信息，我们不妨思考一个信息传递问题。

这个问题可以这样描述：假设有A向B传递一条完全由英文写成的消息，B对A要发送的信息一无所知，如何确定A向B具体传送了多少信息？

如果我们将信息定义为对不确定性的度量，那么B现在对A所发出的信息有多么不确定？首先，英文字母一共有26个，假设A一共发送了N个字母且每一个字母出现概率是等可能的话，那么B可能会收到$26^N$种不同的消息，且每条消息被发出的概率都是$P(N)=\frac{1}{26^N}$。我们可以看到B对于将要接收到的信息具有巨大的不确定性。

每当B从A接收到一个字母，最终接收到的字符串的可能的组合都会减少，假设B已经收到i个字母，那么B最终收到的字符串的可能的组合减少为 $26^{N-i}$。这意味着每当B接受到了一个字母，B对于消息的不确定性就会减少。具体而言，组合数减少了 $26^i=\frac{26^{N}}{26^{N-i}}$，而不确定性也等量的减少了。因此我们可以从组合的角度来度量不确定性和信息。

但是如果我们使用组合数去定义不确定性的话，可能会在计算上十分的麻烦，不妨加上一个 $\log$。更具体一点，假设B收到了消息“abc”，我们可以直接计算B减少的不确定性也即 
$$
I(abc)=-\log P(abc)=-\log \frac{1}{26^3}=i\log26
$$
其中$I()$表示信息。

这样的设置有诸多的好处，例如我们可以直接使用加法计算收到“a”然后再收到“b”的减少的不确定性，即$I(ab)=I(a)+I(b)=-\log P(a)-\log P(b)=-\log P(ab)$。

再来做进一步的思考，尽管我们假设B收到了消息“abc”，但这不是B唯一可能接收到的信息。我们应当平均考虑B能够接收到的消息的所有可能性，也即B所消除的不确定性的期望来作为信息，即：$I(X)=E(X)=-P(X)\log{P(X)}=\frac{1}{26^N}N\log26$，其中N为收到的消息X的长度。

从这里，我们就可以直观的理解为何香农要使用概率和组合学的方式去定义信息了。每一条消息的发送，我们都可以认为是消除了接收者对于消息的不确定性，而接收者对于信息的不确定恰好可以用消息所有可能的组合或者说消息的概率去进行度量。

好了，现在不管从那个角度来看，香农的信息论都看起来天衣无缝，不确定性似乎是信息的天然度量。

但是，不确定性真的就是**信息**这个词的全部了吗？

柯尔莫戈洛夫对此持有不同态度，因为他认为信息的本质实际上是**随机性**。

随机性和不确定性的含义似乎非常接近，但是两者并非完全等价的关系。当我们考虑不确定性的时候，我们其实是在收信人的两个状态之间进行比较，比如B收到信息之前和B收到信息之后。但是随机性实际上是与收信人的状态无关的，任意一个物体处于任意状态下，我们都可以直观的去感受其随机性。比方说，我们一定不会认为“aaaaaa”这个字符串的随机性大于“gfsedh”。

但是为什么会这样？为了直观的理解随机性，我们不妨做一个关于无限随机猴子的思维实验。

现在假设有一个猴子被锁在一台电脑前，他所能对这台电脑做出的唯一的事情就是完全随机的敲打键盘。那么，如果经过无限长的时间，这只猴子将可以在这台电脑上打印出宇宙中所有可能的字符串。

但是请考虑这样一个问题，这只猴子是有更可能打印出"aaaaaaaaa"还是更有可能打印出"fbaecdasc"？由于这只猴子在打字的时候是完全随机的（也即敲击每个字符的概率相等），那么从概率论的角度来看，这两个字符串产生的可能是相等的，同时从经典信息论的角度来看，我们收到的信息也是相等的。

但是从直觉的角度来看，这个结论让人不能接受。因为根据这个结论，在完全随机的情况下，无限猴子打印出整篇《哈姆雷特》的可能性质和打印出一段等长的随机字符串乱码是一样的。而根据经典信息论，这两段字符串提供的信息也是相等的。而这也被称为“无限猴子悖论”。

这是怎么回事？我们的直觉，我们的大脑告诉我们：'aaaaaaaaa'和'fbaecdasc'提供了不同的**信息**，但是经典的信息论又告诉我们这两者之间包含的信息相同。

而这就是柯尔莫戈洛夫为什么会认为用概率来定义信息是不完全正确的。他认为经典信息论是反直觉的。

为了提供一个真正符合直觉的信息论，柯尔莫戈洛夫使用压缩的方法来重新定义信息。他认为压缩**代表着去除原有消息中的所有冗余**。当我们将一条消息压缩到最极限，最后留下的内容就可以代表原有消息中真正的信息。我们不妨这样总结他的观点：“**信息就是对消息进行压缩之后剩下的东西。**”

为什么说从这个角度定义的信息实际上是从随机性的角度定义？这是因为一条消息的随机性与其可压缩性是相等的。一段消息越是可压缩，它就越不随机，同样的道理，如果一条消息完全是随机的，那么我们将无法压缩它。

让我们尝试使用python语言来尝试压缩之前的两个字符串。假设我们在收到这两段字符串之后尝试对他们进行压缩，那么对于"aaaaaaaaa"，我们可以将其压缩为"a"*9，而对于"fbaecdasc"，我们除了直接保留其原本的状态别无他法。

为什么会这样？这其实是因为我们在"aaaaaaaaa"这个字符串中找到了其内部的模式或者说规律，我们利用了这一点来进行压缩，而"fbaecdasc"并没有明显可以利用的模式或者规律可以利用，所以也就无法被压缩。

从这个角度，我们就可以理解随机性和压缩的关系：一段消息越随机，就越缺乏规律，缺乏内在的模式，也就越难以被压缩。从随机性的角度来讲，越有规律的字符串其信息就越少，因为我们可以将其压缩到更短的状态。

有了从随机性出发重新定义的信息，我们终于可以解决信息论中的无限猴子悖论了。

我们可以说，莎士比亚的《哈姆雷特》相较于等长的字符串乱码包含了更多的规律、更多的模式、更小的随机性，因此也就包含更少的信息。

为了帮助建立有关随机性的直觉，我们我们不妨思考一个问题：背住整本《哈姆雷特》更难，还是背住一段与其等长的随机字符串乱码更难？

尽管两者都有难度，但是毫无疑问的是后者更难。这实际上是由于后者包含的随机性更大，信息量更大，更难以被记忆。也就是说：一段话包含的信息越多，我们就越难以记住它。实际上是因为我们的大脑是一个出色的压缩器，它总是通过压缩的方法来记忆信息，越是随机，信息量就越大，也就越难以记住。

但是这里仍然有一件反直觉的事情，那就是会有人认为更多的模式意味着更多的信息。那么此时，这些人实际上是以**随机性的倒数**来理解信息。这从本质上来看依旧是从随机性的角度去进行度量。

从数学的角度来看，这种看法与直接使用随机性来度量信息并无重大区别，但是在运算上可能会造成困难，所以我们还是采用最原本的随机性的角度来理解信息。


## 柯氏复杂度

从压缩和随机性的角度来看待信息看起来要比经典信息论更合理一点，但是我们如何定量的从这个角度来描述信息？

我们刚刚提到，一条消息中的规律和模式与其包含的信息量有关。所以我们需要找到一个可以度量规律和模式的方法来对信息进行度量。

这时我们终于可以介绍柯氏复杂度了，柯氏复杂度即是柯尔莫戈洛夫用来度量信息的数学工具。这种复杂度利用程序和图灵机来定义数据中的模式，可以说柯氏复杂度是从计算的角度定义了信息。

柯氏复杂度的正式定义为：可以输出X的最短程序P的长度。

用更形式化的语言可以表述为：

$$
K_U(X)=min\{length(P):U(P)=X\}
$$  

其中$U$表示图灵机（或者直白点，一台计算机），$P$表示对应的可以运行在$U$上输出$X$的程序。

但是这里还有一个问题，我们为什么需要使用程序和图灵机来定义模式和规律？

这其实是一个有争议的做法，但是我们可以有一些合理的解释。

首先，这对于随机事件X的具体内容是没有影响的。尽管随机事件是各式各样的，但是我们总是能将其编码成可以计算机程序可识别的样子。


（无限猴子本身就被锁在电脑前！）
程序能否表示任意随机事件中存在的任意模式？


### 数学性质

#### 不变性定理

#### 条件复杂性

### 与信息熵的关系

### 作为一种压缩的度量






