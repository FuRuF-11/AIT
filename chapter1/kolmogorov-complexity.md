# Kolmogorov Complexity

## 前言

要想了解AIT，就要了解[柯氏复杂度](https://en.wikipedia.org/wiki/Kolmogorov_complexity)。

在上个世纪，有三位数学家（[所罗门诺夫](https://en.wikipedia.org/wiki/Ray_Solomonoff)，[柯尔莫戈洛夫](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)，[柴廷](https://en.wikipedia.org/wiki/Gregory_Chaitin)）分别在不同的时间点上独立的发现了柯氏复杂度（全称为柯尔莫戈洛夫复杂度，Kolmogorov Complexity，或者称为算法复杂度、描述复杂度、所罗门诺夫复杂度、柴廷常数）这个算法信息论中最核心的概念。

这三位数学家的出发点各不相同，但是从柯尔莫戈洛夫的视角出发，会更容易理解这个概念。

柯尔莫戈洛夫作为上世纪最伟大的苏联数学家，曾长久的为“信息的本质”这个问题所困扰。经过漫长的思考，柯尔莫戈洛夫在1963年左右得出了最终的结论。他认为，我们可以从三种不同的角度定义和理解信息的本质:
1. 组合学
2. 概率论
3. 算法 

其中，从组合学和概率论的角度出发，我们可以得到香农的经典信息论，也即从不确定性的角度来度量信息。而从算法的角度出发，我们可以得到柯氏复杂度。柯尔莫戈洛夫认为这个角度实际上是从随机性的角度来定义信息。同时他也认为，从这个角度出发得到的信息的定义是最好的。

## 经典信息论vs算法信息论

克劳德·香农在1948年发表了[《通信的数学原理》](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)，创立了经典信息论。从那个时刻开始，人们认识到信息不是一种虚无飘渺的东西。在香农的信息论中，信息就如同电压一样，是一种可以定量计算、有实际物理含义的物理量。香农认为，信息本质上是不确定性的一种体现。他用信息熵来定义信息。对于一个离散的随机变量$X$，其信息熵由如下数学公式进行计算：

$$
H(X):=-\sum_{x \in X}P(x)logP(x)
$$

对于连续的随机变量则需要使用积分的方法计算。但是不论如何，信息熵都是在随机变量的分布上进行计算的。所以从直观上我们很容易理解为何信息熵是与不确定性和概率相关的，因为信息熵需要使用随机变量不同取值下的概率进行计算。

为了能够更加直观的理解信息熵，也为了我们从组合的角度去理解信息，我们不妨思考一个信息传递问题。

这个问题可以这样描述：假设有A向B传递一条完全由英文写成的消息，B对A要发送的信息一无所知，如何确定A向B具体传送了多少信息？

如果我们将信息定义为对不确定性的度量，那么B现在对A所发出的信息有多么不确定？首先，英文字母一共有26个，假设A一共发送了N个字母且每一个字母出现概率是等可能的话，那么B可能会收到$26^N$种不同的消息，且每条消息被发出的概率都是$P(N)=\frac{1}{26^N}$。我们可以看到B对于将要接收到的信息具有巨大的不确定性。

每当B从A接收到一个字母，最终接收到的字符串的可能的组合都会减少，假设B已经收到i个字母，那么B最终收到的字符串的可能的组合减少为 $26^{N-i}$。这意味着每当B接受到了一个字母，B对于消息的不确定性就会减少。具体而言，组合数减少了 $26^i=\frac{26^{N}}{26^{N-i}}$，而不确定性也等量的减少了。因此我们可以从组合的角度来度量不确定性和信息。

但是如果我们使用组合数去定义不确定性的话，可能会在计算上十分的麻烦，不妨加上一个 $\log$。更具体一点，假设B收到了消息“abc”，我们可以直接计算B减少的不确定性也即 
$$
I(abc)=-\log P(abc)=-\log \frac{1}{26^3}=i\log26
$$
其中$I()$表示信息。

这样的设置有诸多的好处，例如我们可以直接使用加法计算收到“a”然后再收到“b”的减少的不确定性，即$I(ab)=I(a)+I(b)=-\log P(a)-\log P(b)=-\log P(ab)$。

再来做进一步的思考，尽管我们假设B收到了消息“abc”，但这不是B唯一可能接收到的信息。我们应当平均考虑B能够接收到的消息的所有可能性，也即B所消除的不确定性的期望来作为信息，即：$I(X)=E(X)=-P(X)\log{P(X)}=\frac{1}{26^N}N\log26$，其中N为收到的消息X的长度。

从这里，我们就可以直观的理解为何香农要使用概率和组合学的方式去定义信息了。每一条消息的发送，我们都可以认为是消除了接收者对于消息的不确定性，而接收者对于信息的不确定恰好可以用消息所有可能的组合或者说消息的概率去进行度量。

好了，现在不管从那个角度来看，香农的信息论都看起来天衣无缝，不确定性似乎是信息的天然度量。

但是，不确定性真的就是**信息**这个词的全部了吗？

柯尔莫戈洛夫对此持有不同态度，因为他认为信息的本质实际上是**随机性**。

随机性和不确定性的含义似乎非常接近，但是两者并非完全等价的关系。当我们考虑不确定性的时候，我们其实是在收信人的两个状态之间进行比较，比如B收到信息之前和B收到信息之后。但是随机性实际上是与收信人的状态无关的，任意一个物体处于任意状态下，我们都可以直观的去感受其随机性。比方说，我们一定不会认为“aaaaaa”这个字符串的随机性大于“gfsedh”。

但是为什么会这样？为了直观的理解随机性，我们不妨做一个关于无限随机猴子的思维实验。

现在假设有一个猴子被锁在一台电脑前，他所能对这台电脑做出的唯一的事情就是完全随机的敲打键盘。那么，如果经过无限长的时间，这只猴子将可以在这台电脑上打印出宇宙中所有可能的字符串。

但是请考虑这样一个问题，这只猴子是有更可能打印出"aaaaaaaaa"还是更有可能打印出"fbaecdasc"？由于这只猴子在打字的时候是完全随机的（也即敲击每个字符的概率相等），那么从概率论的角度来看，这两个字符串产生的可能是相等的，同时从经典信息论的角度来看，我们收到的信息也是相等的。

但是从直觉的角度来看，这个结论让人不能接受。因为根据这个结论，在完全随机的情况下，无限猴子打印出整篇《哈姆雷特》的可能性质和打印出一段等长的随机字符串乱码是一样的。而根据经典信息论，这两段字符串提供的信息也是相等的。而这也被称为“无限猴子悖论”。

这是怎么回事？我们的直觉，我们的大脑告诉我们：'aaaaaaaaa'和'fbaecdasc'提供了不同的**信息**，但是经典的信息论又告诉我们这两者之间包含的信息相同。

而这就是柯尔莫戈洛夫为什么会认为用概率来定义信息是不完全正确的。他认为经典信息论是反直觉的。

为了提供一个真正符合直觉的信息论，柯尔莫戈洛夫使用压缩的方法来重新定义信息。他认为压缩**代表着去除原有消息中的所有冗余**。当我们将一条消息压缩到最极限，最后留下的内容就可以代表原有消息中真正的信息。我们不妨这样总结他的观点：“**信息就是对消息进行压缩之后剩下的东西。**”

为什么说从这个角度定义的信息实际上是从随机性的角度定义？这是因为一条消息的随机性与其可压缩性是相等的。一段消息越是可压缩，它就越不随机，同样的道理，如果一条消息完全是随机的，那么我们将无法压缩它。

让我们尝试使用python语言来尝试压缩之前的两个字符串。假设我们在收到这两段字符串之后尝试对他们进行压缩，那么对于"aaaaaaaaa"，我们可以将其压缩为"a"*9，而对于"fbaecdasc"，我们除了直接保留其原本的状态别无他法。

为什么会这样？这其实是因为我们在"aaaaaaaaa"这个字符串中找到了其内部的模式或者说规律，我们利用了这一点来进行压缩，而"fbaecdasc"并没有明显可以利用的模式或者规律可以利用，所以也就无法被压缩。

从这个角度，我们就可以理解随机性和压缩的关系：一段消息越随机，就越缺乏规律，缺乏内在的模式，也就越难以被压缩。从随机性的角度来讲，越有规律的字符串其信息就越少，因为我们可以将其压缩到更短的状态。

有了从随机性出发重新定义的信息，我们终于可以解决信息论中的无限猴子悖论了。

我们可以说，莎士比亚的《哈姆雷特》相较于等长的字符串乱码包含了更多的规律、更多的模式、更小的随机性，因此也就包含更少的信息。

为了帮助建立有关随机性的直觉，我们我们不妨思考一个问题：背住整本《哈姆雷特》更难，还是背住一段与其等长的随机字符串乱码更难？

尽管两者都有难度，但是毫无疑问的是后者更难。这实际上是由于后者包含的随机性更大，信息量更大，更难以被记忆。也就是说：一段话包含的信息越多，我们就越难以记住它。实际上是因为我们的大脑是一个出色的压缩器，它总是通过压缩的方法来记忆信息，越是随机，信息量就越大，也就越难以记住。

但是这里仍然有一件反直觉的事情，那就是会有人认为更多的模式意味着更多的信息。那么此时，这些人实际上是以**随机性的倒数**来理解信息。这从本质上来看依旧是从随机性的角度去进行度量。

从数学的角度来看，这种看法与直接使用随机性来度量信息并无重大区别，但是在运算上可能会造成困难，所以我们还是采用最原本的随机性的角度来理解信息。


## 柯氏复杂度

从压缩和随机性的角度来看待信息看起来要比经典信息论更合理一点，但是我们如何定量的从这个角度来描述信息？

我们刚刚提到，一条消息中的规律和模式与其包含的信息量有关。所以我们需要找到一个可以度量规律和模式的方法来对信息进行度量。

这时我们终于可以介绍柯氏复杂度了，柯氏复杂度即是柯尔莫戈洛夫用来度量信息的数学工具。这种复杂度利用程序和图灵机来定义数据中的模式，可以说柯氏复杂度是从计算的角度定义了信息。

柯氏复杂度的正式定义为：可以输出X的最短程序P的长度。

用更形式化的语言可以表述为：

$$
K_U(X)=min\{length(P):U(P)=X\}
$$  

其中$U$表示图灵机（或者直白点，一台计算机），$P$表示对应的可以运行在$U$上输出$X$的程序。

为什么需要使用程序来定义模式和规律？从刚刚我们提到的无限猴子的角度来解释话，我们可以说猴子敲打的结果也是一种程序。一个例子就是这只猴子可能恰好在电脑上打出了一个图形库来输出各种图形。所以我们看到的所有随机输出的结果都是某种程序在电脑（图灵机）上运行的结果。

使用程序来描述实际上还是有些绕弯子，我们不妨使用“步骤”这个词来进行思考。实际上程序只是一些固定步骤的排列组合。当我们使用步骤这个词汇的时候，我们可以理解到为什么柯氏复杂度是一个客观的概念。我们宇宙中所有事件的发生都是有原因的，不论是多么复杂的事件，都必须经过一系列的步骤才能达成。

同时，我们每个人都可以直观的理解：如果一个事件仅需要少量的步骤就可以发生的话，这个事件就不会太复杂，也就不会带有太多的信息。如果一个事件的发生需要相当多的步骤的话，那么这个事件背后的信息量也会非常的大。

但是也许你会说，对于同一个事件，我们可以将其分割成同样数量的不同步骤或者将这个事件分割成数量上比较接近的多个步骤，这会对事件的复杂度产生影响吗？

这时候，我们又需要从程序的角度去思考柯氏复杂度了。通过从程序的角度来思考，我们可以得到柯氏复杂度的一个重要性质：不变性定理。

### 不变性定理

不变性定理是柯氏复杂度最关键的性质，这个性质保证了柯氏复杂度的客观性。

不变性定理是说：不同的图灵机上同一对象X的柯氏复杂度之间仅有相差了一个常数。用数学化的语言可以表示为：  
$$
\forall X; |K_U(X)-K_V(X)| <C_{U,V}
$$

其中，$U$和$V$表示不同的图灵机。可以看到，$C$是一个仅取决于$U$和$V$的常数。

从程序的角度证明这个定理非常简单，你只需要想象有一个编译器将一种语言转化成另一种语言，而这种编译器的长度一定是固定的。

不变性定理是一个柯氏复杂度非常重要的性质。不变性定理告诉我们，不同的图灵机上运行的程序之间仅会相差一个常数。这使得我们可以选取一个特定的图灵机来讨论柯氏复杂度。

同时，相差一个常数的事实告诉我们，不同的图灵机上的柯氏复杂度存在序列关系。

所以，对于任意的一个对象，我们总可以找到一个特殊的图灵机$U$，使得$U$上的柯氏复杂度相对于在其他图灵机$V$上的更小：

$$
K_{U}(X) \leq K_{V}(X)
$$

### 不可计算性

柯氏复杂度的另一个重要的性质是不可计算性。这种性质意味着我们永远都无法找到可以计算柯氏复杂度的程序（但是我们可以在有限的时间内逼近它）。

根据香农的编码定理，一个对象$X$，他的柯氏复杂度由以下不等式进行约束：

$$
K(X) \leq \log_2(P(X))+O(1)
$$

即使我们在接下来可以用这个不等式得到柯氏复杂度的上界，我们也无法通过穷举的方式得到可以输出$X$的最短的程序。由于停机问题是无法解决的，某些程序可能永远都不会结束。正是因此，我们永远无法验证一个程序是否是我们理想的最短程序。

<!-- 这条性质被很多人视为柯氏复杂度不完备性的体现。但是恰恰相反，这是柯氏复杂度完备性的体现。 -->

### 条件柯氏复杂度与联合柯氏复杂度

柯氏复杂度作为一种信息的度量，与传统的信息熵有很多类似的性质。柯氏复杂度同样可以一个对象为条件来计算另一个对象的条件柯氏复杂度也可以计算多个对象的联合柯氏复杂度。

仿照柯氏复杂度的定义，对于对象间的条件柯氏复杂度，我们有定义：

$$
K(s_2|s_1)=min_P\{ length(P)| U(s_1,P)=s_2 \}
$$

直观来看，以$s_1$为条件的$s_2$的柯氏复杂度，是利用$s_1$作为一个前提去找到可以输出$s_2$的程序。这种情况下，我们可以说$s_1$是这个程序的输入。

同样的，对于两个对象之间的联合柯氏复杂度，我们有：

$$
K(s_1,s_2)=min_P\{ length(P)| U(P)=s_1+s_2 \}
$$

同时，我们可以得到两条关于条件柯氏复杂度和联合柯氏复杂度的性质:

$$
K(s_2) \leq K(s_1)+K(s_2|s_1)+O(1) \\
K(s_2,s_1) \leq K(s_1)+K(s_2|s_1)+O(1)
$$

这被称为柯氏复杂度的链式法则。

这两条性质的证明方法同样需要我们进行模拟论证。对于第一条链式法则，如果我们有给定$s1$可以生成$s2$的程序$P_{12}$，同时有可以产生$s_1$的程序$P_1$，那么产生s2的程序的长度至少应该小于等于$P_{21}+P_{2}+O(1)$，其中$O(1)$常数是由于选取的图灵机不同而产生的，将两段程序拼接在一起产生的代价。

仿照这段论证，我们也可以证明第二条性质。

## 结论

柯氏复杂度作为算法信息论的最基础概念，是对于传统信息理论的一次革新。通过柯氏复杂度，我们能从压缩和随机性的视角来看待信息的本质。

在下一个章节，我们会发现，通过柯氏复杂度，我们还可以定义概率的本质。



