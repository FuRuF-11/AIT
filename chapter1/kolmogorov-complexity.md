# Kolmogorov Complexity

## 前言

要想了解AIT，就要了解[柯氏复杂度](https://en.wikipedia.org/wiki/Kolmogorov_complexity)。

在上个世纪，有三位数学家（[所罗门诺夫](https://en.wikipedia.org/wiki/Ray_Solomonoff)，[柯尔莫戈洛夫](https://en.wikipedia.org/wiki/Andrey_Kolmogorov)，[柴廷](https://en.wikipedia.org/wiki/Gregory_Chaitin)）分别在不同的时间点上独立的发现了柯氏复杂度（全称为柯尔莫戈洛夫复杂度，Kolmogorov Complexity，或者称为算法复杂度、描述复杂度、所罗门诺夫复杂度、柴廷常数）这个算法信息论中最核心的概念。

这三位数学家的出发点各不相同，但是从柯尔莫戈洛夫的视角出发，会更容易理解这个概念。

柯尔莫戈洛夫作为上世纪最伟大的苏联数学家，曾长久的为“信息的本质”这个问题所困扰。经过漫长的思考，柯尔莫戈洛夫在1963年左右得出了最终的结论。他认为，我们可以从三种不同的角度定义和理解信息的本质:
1. 组合学
2. 概率论
3. 算法 

其中，从组合学和概率论的角度出发，我们可以得到香农的经典信息论，也即从不确定性的角度来度量信息。而从算法的角度出发，我们可以得到柯氏复杂度。柯尔莫戈洛夫认为这个角度实际上是从随机性的角度来定义信息。同时他也认为，从这个角度出发得到的信息的定义是最好的。

## 经典信息论vs算法信息论

克劳德·香农在1948年发表了[《通信的数学原理》](https://en.wikipedia.org/wiki/A_Mathematical_Theory_of_Communication)，创立了经典信息论。从那个时刻开始，人们认识到信息不是一种虚无飘渺的东西。在香农的信息论中，信息就如同电压一样，是一种可以定量计算、有实际物理含义的物理量。香农认为，信息本质上是不确定性的一种体现。他用信息熵来定义信息。对于一个离散的随机变量$X$，其信息熵由如下数学公式进行计算：

$$
H(X):=-\sum_{x \in X}P(x)logP(x)
$$

对于连续的随机变量则需要使用积分的方法计算。但是不论如何，信息熵都是在随机变量的分布上进行计算的。所以从直观上我们很容易理解为何信息熵是与不确定性和概率相关的，因为信息熵需要使用随机变量不同取值下的概率进行计算。

为了能够更加直观的理解信息熵，也为了我们从组合的角度去理解信息，我们不妨思考一个信息传递问题。

这个问题可以这样描述：假设有A向B传递一条完全由英文写成的消息，B对A要发送的信息一无所知，如何确定A向B具体传送了多少信息？

如果我们将信息定义为对不确定性的度量，那么B现在对A所发出的信息有多么不确定？首先，英文字母一共有26个，假设A一共发送了N个字母且每一个字母出现概率是等可能的话，那么B可能会收到$26^N$种不同的消息，且每条消息被发出的概率都是$P(N)=\frac{1}{26^N}$。我们可以看到B对于将要接收到的信息具有巨大的不确定性。

每当B从A接收到一个字母，最终接收到的字符串的可能的组合都会减少，假设B已经收到i个字母，那么B最终收到的字符串的可能的组合减少为$26^{N-i}$。这意味着每当B接受到了一个字母，B对于消息的不确定性就会减少。具体而言，组合数减少了$26^i=\frac{26^{N}}{26^{N-i}}$。

如果我们使用可能的组合数去定义不确定性的话，可能会十分的麻烦，不妨加上一个$\log$，我们可以得到一个易于计算的结果$i\log26$。由此我们可以看到，B可以将会收到消息的不确定性的期望作为他收到的信息的一种度量$E(N)=-P(N)\log{P(N)}=\frac{1}{26^N}N\log26$。

从这里，我们就可以直观的理解为何香农要使用概率和组合学的方式去定义信息了。每一条消息的发送，我们都可以认为是消除了接收者对于消息的不确定性，而接收者对于信息的不确定恰好可以用消息所有可能的组合或者说消息的概率去进行度量。



